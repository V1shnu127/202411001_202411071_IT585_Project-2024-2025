{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1553396,"sourceType":"datasetVersion","datasetId":916809},{"sourceId":3816852,"sourceType":"datasetVersion","datasetId":2273823}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport timm\nimport json\nfrom sklearn.metrics import jaccard_score\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport scipy.io\nimport torch.utils.checkpoint as checkpoint\nimport matplotlib.pyplot as plt\nimport random\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T14:53:06.751782Z","iopub.execute_input":"2025-05-04T14:53:06.752088Z","iopub.status.idle":"2025-05-04T14:53:06.756617Z","shell.execute_reply.started":"2025-05-04T14:53:06.752068Z","shell.execute_reply":"2025-05-04T14:53:06.756064Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# Set environment variable to help with memory fragmentation\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\n# Define paths\nLVIS_BASE_PATH = '/kaggle/input/lvis-v1'\nBERKELEY_BASE_PATH = '/kaggle/input/berkeley-segmentation-dataset-500-bsds500'\nOUTPUT_DIR = '/kaggle/working/output'\nos.makedirs(OUTPUT_DIR, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T14:53:10.980947Z","iopub.execute_input":"2025-05-04T14:53:10.981539Z","iopub.status.idle":"2025-05-04T14:53:10.985231Z","shell.execute_reply.started":"2025-05-04T14:53:10.981514Z","shell.execute_reply":"2025-05-04T14:53:10.984624Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# Dataset class for LVIS (training only)\nclass LVISDataset(Dataset):\n    def __init__(self, image_dir, ann_file, transform=None):\n        self.image_dir = image_dir\n        with open(ann_file, 'r') as f:\n            self.annotations = json.load(f)['annotations']\n        self.transform = transform\n\n        # Filter image_ids to only include images that exist in the directory\n        all_image_ids = list(set(ann['image_id'] for ann in self.annotations))\n        self.image_ids = []\n        for img_id in all_image_ids:\n            img_path = os.path.join(self.image_dir, f'{img_id:012d}.jpg')\n            if os.path.exists(img_path):\n                self.image_ids.append(img_id)\n            else:\n                print(f\"Warning: Image {img_path} not found, skipping.\")\n\n        # Use only 1/20th of the dataset\n        self.image_ids = self.image_ids[:len(self.image_ids) // 20]\n        print(f\"Total images available for training (1/4th): {len(self.image_ids)}\")\n\n    def __len__(self):\n        return len(self.image_ids)\n\n    def __getitem__(self, idx):\n        img_id = self.image_ids[idx]\n        img_path = os.path.join(self.image_dir, f'{img_id:012d}.jpg')\n        image = cv2.imread(img_path)\n        if image is None:\n            raise FileNotFoundError(f\"Image not found at {img_path}\")\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        # Create mask (binary: foreground=1, background=0)\n        mask = np.zeros(image.shape[:2], dtype=np.uint8)\n        for ann in self.annotations:\n            if ann['image_id'] == img_id:\n                seg = ann['segmentation']\n                for poly in seg:\n                    poly = np.array(poly).reshape(-1, 2).astype(np.int32)\n                    cv2.fillPoly(mask, [poly], 1)\n\n        if self.transform:\n            augmented = self.transform(image=image, mask=mask)\n            image, mask = augmented['image'], augmented['mask']\n\n        return image, mask\n\n# Dataset class for Berkeley (test only)\nclass BerkeleyDataset(Dataset):\n    def __init__(self, base_path, split='test', transform=None):\n        self.split = split\n        self.image_dir = os.path.join(base_path, 'images', split)\n        self.mask_dir = os.path.join(base_path, 'ground_truth', split)\n        self.transform = transform\n        self.images = [f for f in os.listdir(self.image_dir) if f.endswith('.jpg')]\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img_name = self.images[idx]\n        img_path = os.path.join(self.image_dir, img_name)\n        mask_name = img_name.replace('.jpg', '.mat')\n        mask_path = os.path.join(self.mask_dir, mask_name)\n        \n        # Load image\n        image = cv2.imread(img_path)\n        if image is None:\n            raise FileNotFoundError(f\"Image not found at {img_path}\")\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        # Load mask from .mat file\n        mat = scipy.io.loadmat(mask_path)\n        mask = mat['groundTruth'][0, 0]['Segmentation'][0, 0]\n        mask = (mask > 0).astype(np.uint8)\n\n        if self.transform:\n            augmented = self.transform(image=image, mask=mask)\n            image, mask = augmented['image'], augmented['mask']\n\n        return image, mask, img_name","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T16:02:20.452500Z","iopub.execute_input":"2025-05-04T16:02:20.453019Z","iopub.status.idle":"2025-05-04T16:02:20.464573Z","shell.execute_reply.started":"2025-05-04T16:02:20.452993Z","shell.execute_reply":"2025-05-04T16:02:20.463921Z"}},"outputs":[{"name":"stdout","text":"ERROR! Session/line number was not unique in database. History logging moved to new session 30\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"# Data transforms (224x224 to match ViT-B expectation)\ntrain_transform = A.Compose([\n    A.Resize(224, 224),\n    A.HorizontalFlip(p=0.5),\n    A.Rotate(limit=30, p=0.5),\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2(),\n])\n\ntest_transform = A.Compose([\n    A.Resize(224, 224),\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2(),\n])\n\n# Probability map modulation\ndef modulate_probability_map(prob_map, gamma=2.0, threshold=0.7):\n    modulated = prob_map.clone()\n    fg_mask = prob_map > threshold\n    modulated[fg_mask] = torch.pow(modulated[fg_mask], 1/gamma)\n    bg_mask = prob_map < (1 - threshold)\n    modulated[bg_mask] = torch.pow(modulated[bg_mask], gamma)\n    return modulated\n\n# MFP Network (ViT-B only)\nclass MFPNet(nn.Module):\n    def __init__(self):\n        super(MFPNet, self).__init__()\n        # ViT-B backbone\n        self.encoder = timm.create_model('vit_base_patch16_224', pretrained=True)\n        self.feature_channels = 768  # ViT-B has 768 channels\n        self.patch_size = 16\n\n        # Probability feature extractor\n        self.prob_conv = nn.Sequential(\n            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n        )\n\n        # Fusion and segmentation head\n        self.fusion = nn.Sequential(\n            nn.Conv2d(self.feature_channels + 64, 256, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(),\n        )\n        self.seg_head = nn.Conv2d(256, 1, kernel_size=1)\n\n    def forward(self, image, prob_map=None):\n        # Backbone features (ViT-B)\n        x = self.encoder.forward_features(image)  # [batch_size, 197, 768]\n        x = x[:, 1:]  # Remove CLS token: [batch_size, 196, 768]\n        # Reshape to feature map: 196 patches = 14x14 grid for 224x224\n        grid_size = int((x.shape[1]) ** 0.5)  # 14 for 224x224\n        x = x.transpose(1, 2).reshape(-1, self.feature_channels, grid_size, grid_size)  # [batch_size, 768, 14, 14]\n\n        # If no prob_map provided, initialize with zeros\n        if prob_map is None:\n            prob_map = torch.zeros((image.shape[0], 1, image.shape[2], image.shape[3]), device=image.device)\n\n        # Modulate probability map\n        modulated_prob = modulate_probability_map(prob_map)\n\n        # Extract probability features\n        prob_features = checkpoint.checkpoint_sequential(self.prob_conv, segments=2, input=modulated_prob)  # [batch_size, 64, 224, 224]\n\n        # Upsample backbone features to match prob_features size (224x224)\n        x = F.interpolate(x, size=prob_features.shape[2:], mode='bilinear', align_corners=False)\n\n        # Late fusion\n        fused = torch.cat([x, prob_features], dim=1)\n        fused = checkpoint.checkpoint_sequential(self.fusion, segments=2, input=fused)\n\n        # Segmentation head\n        logits = self.seg_head(fused)\n        prob = torch.sigmoid(logits)\n        return prob\n\n# Dice loss\nclass DiceLoss(nn.Module):\n    def __init__(self):\n        super(DiceLoss, self).__init__()\n\n    def forward(self, pred, target, smooth=1):\n        pred = pred.contiguous().view(-1)\n        target = target.contiguous().view(-1)\n        intersection = (pred * target).sum()\n        return 1 - ((2. * intersection + smooth) / (pred.sum() + target.sum() + smooth))\n\n# Training function (with accuracy tracking and tqdm progress bar)\ndef train_model(model, train_loader, num_epochs=5, device='cuda'):\n    model.to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n    criterion_ce = nn.BCELoss()\n    criterion_dice = DiceLoss()\n\n    train_accuracies = []\n    for epoch in range(num_epochs):\n        model.train()\n        train_loss = 0\n        train_acc = 0\n        # Wrap the train_loader with tqdm for a progress bar\n        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False)\n        for images, masks in progress_bar:\n            images, masks = images.to(device), masks.to(device).float()\n            masks = masks.unsqueeze(1)\n\n            prob = model(images)\n            prob = model(images, prob)\n\n            loss = 0.5 * criterion_ce(prob, masks) + 0.5 * criterion_dice(prob, masks)\n            train_loss += loss.item()\n\n            # Compute accuracy\n            pred = (prob > 0.5).float()\n            acc = (pred == masks).float().mean().item()\n            train_acc += acc\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            # Update progress bar with current loss and accuracy\n            progress_bar.set_postfix({'loss': f'{loss.item():.4f}', 'acc': f'{acc:.4f}'})\n\n        epoch_acc = train_acc / len(train_loader)\n        train_accuracies.append(epoch_acc)\n        print(f'Epoch {epoch+1}/{num_epochs}, Train Accuracy: {epoch_acc:.4f}')\n\n        # Save model\n        torch.save(model.state_dict(), os.path.join(OUTPUT_DIR, f'model_vit_b_epoch{epoch+1}.pth'))\n\n    return train_accuracies\n\n# Evaluation and visualization function\ndef evaluate_and_visualize(model, test_loader, device='cuda'):\n    model.eval()\n    miou = 0\n    acc = 0\n    # Select a random image for visualization\n    test_dataset = test_loader.dataset\n    random_idx = random.randint(0, len(test_dataset) - 1)\n    image, mask, img_name = test_dataset[random_idx]\n    image = image.unsqueeze(0).to(device)  # Add batch dimension\n    mask = mask.unsqueeze(0).to(device).float()  # Add batch dimension\n\n    with torch.no_grad():\n        # Compute metrics on the entire test set\n        for images, masks, _ in test_loader:\n            images, masks = images.to(device), masks.to(device).float()\n            masks = masks.unsqueeze(1)\n\n            prob = model(images)\n            pred = (prob > 0.5).float()\n            miou += jaccard_score(masks.cpu().numpy().flatten(), pred.cpu().numpy().flatten(), average='binary')\n            acc += (pred == masks).float().mean().item()\n\n        # Predict on the random image\n        prob = model(image)\n        pred = (prob > 0.5).float()\n\n    test_miou = miou / len(test_loader)\n    test_acc = acc / len(test_loader)\n\n    # Visualize the results\n    # Denormalize the image for display\n    image = image.squeeze(0).cpu().numpy().transpose(1, 2, 0)  # [3, 224, 224] -> [224, 224, 3]\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    image = (image * std + mean) * 255\n    image = image.astype(np.uint8)\n\n    mask = mask.squeeze().cpu().numpy()  # [1, 224, 224] -> [224, 224]\n    pred = pred.squeeze().cpu().numpy()  # [1, 224, 224] -> [224, 224]\n\n    # Plot\n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n    axes[0].imshow(image)\n    axes[0].set_title(f\"Input Image: {img_name}\")\n    axes[0].axis('off')\n\n    axes[1].imshow(mask, cmap='gray')\n    axes[1].set_title(\"Ground Truth Mask\")\n    axes[1].axis('off')\n\n    axes[2].imshow(pred, cmap='gray')\n    axes[2].set_title(\"Predicted Mask (ViT-B)\")\n    axes[2].axis('off')\n\n    plt.savefig(os.path.join(OUTPUT_DIR, f'segmentation_vit_b_{img_name}.png'))\n    plt.close()\n\n    return test_miou, test_acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T16:03:30.601971Z","iopub.execute_input":"2025-05-04T16:03:30.602804Z","iopub.status.idle":"2025-05-04T16:03:30.625976Z","shell.execute_reply.started":"2025-05-04T16:03:30.602758Z","shell.execute_reply":"2025-05-04T16:03:30.625071Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"# Main execution\ndef main():\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    # Clear GPU memory cache\n    if device.type == 'cuda':\n        torch.cuda.empty_cache()\n\n    # Load datasets\n    train_dataset = LVISDataset(\n        image_dir=os.path.join(LVIS_BASE_PATH, 'train2017', 'train2017'),\n        ann_file=os.path.join(LVIS_BASE_PATH, '/kaggle/input/lvis-v1/lvis_v1_train.json/lvis_v1_train.json'),\n        transform=train_transform\n    )\n    test_dataset = BerkeleyDataset(\n        base_path=BERKELEY_BASE_PATH,\n        split='test',\n        transform=test_transform\n    )\n\n    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=0)\n    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)\n\n    # Train and evaluate with ViT-B\n    print('\\nTraining with ViT-B backbone')\n    model_vit = MFPNet()\n    vit_train_accuracies = train_model(model_vit, train_loader, num_epochs=5, device=device)\n\n    print('\\nEvaluating with ViT-B backbone')\n    vit_test_miou, vit_test_acc = evaluate_and_visualize(model_vit, test_loader, device=device)\n    print(f'ViT-B Test mIoU: {vit_test_miou:.4f}, Test Accuracy: {vit_test_acc:.4f}')\n\n    # Print training accuracies\n    print(\"\\nViT-B Training Accuracies per Epoch:\")\n    for epoch, acc in enumerate(vit_train_accuracies, 1):\n        print(f\"Epoch {epoch}: {acc:.4f}\")\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T16:03:32.875799Z","iopub.execute_input":"2025-05-04T16:03:32.876465Z"}},"outputs":[],"execution_count":null}]}
